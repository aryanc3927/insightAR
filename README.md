Project Description: 

The problem being investigated is how to enhance conversational experiences by providing relevant information about people during discussions in an unobtrusive way. This is interesting because manually recalling details can disrupt conversational flow. The project will examine research in facial recognition, natural language processing, augmented reality, and conversational AI. Publicly available facial recognition and speech datasets will initially be used for training. Real-world conversational data will also be collected from volunteers with proper consent and anonymization processes.


The proposed method integrates Apple's Vision, Speech, Natural Language, RealityKit, and ARKit frameworks. Facial recognition will identify conversation partners. Speech recognition and NLP will transcribe and extract relevant conversation details. Augmented reality overlays will display this information. Secure data storage will log conversation details with user consent for future reference. Results will be evaluated through user testing for qualitative experience assessment. Quantitative metrics include facial recognition accuracy, speech recognition word error rate, NLP entity extraction precision/recall, and response time for information retrieval/display. Logged conversation data accuracy will also be analyzed.


Expected results include visualizations of system performance across the quantitative metrics as well as user feedback summaries. Statistical analysis like precision/recall curves and A/B testing will compare the system's performance against baseline approaches.
